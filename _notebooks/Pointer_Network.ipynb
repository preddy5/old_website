{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a implemention of Pointer Networks in keras(similar to seq2set) http://arxiv.org/pdf/1511.06391v4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cPickle\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras.layers.recurrent import time_distributed_dense\n",
    "from keras.activations import tanh, softmax\n",
    "from keras.layers import Input, LSTM, Dense, RepeatVector, Lambda, Activation\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.engine import InputSpec\n",
    "from keras.models import Model\n",
    "import keras.backend as K \n",
    "from keras.callbacks import Callback, LearningRateScheduler\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointerLSTM\n",
    "\n",
    "This is a custom decoder layer that returns pointer for the input sequence. This is a slight modification to the famous soft attention LSTM paper by Bandanau et al( Neural machine translation by jointly learning to align and translate. In Proc. ICLR, 2015).\n",
    "\n",
    "#### A visualization of what the decoder is doing.\n",
    "\n",
    "<img src =\"Ptr-net.png\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointerLSTM(LSTM):\n",
    "    def __init__(self, hidden_shape, *args, **kwargs):\n",
    "        self.hidden_shape = hidden_shape\n",
    "        self.input_length=[]\n",
    "        super(PointerLSTM, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PointerLSTM, self).build(input_shape)        \n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        init = initializations.get('orthogonal')\n",
    "        self.W1 = init((self.hidden_shape, 1))\n",
    "        self.W2 = init((self.hidden_shape, 1))\n",
    "        self.vt = init((input_shape[1], 1))\n",
    "        self.trainable_weights += [self.W1, self.W2, self.vt]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        en_seq = x\n",
    "        x_input = x[:, input_shape[1]-1, :]\n",
    "        x_input = K.repeat (x_input, input_shape[1])\n",
    "        initial_states = self.get_initial_states(x_input)\n",
    "\n",
    "        constants = super(PointerLSTM, self).get_constants(x_input)\n",
    "        constants.append(en_seq)\n",
    "        preprocessed_input = self.preprocess_input(x_input)\n",
    "\n",
    "        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n",
    "                                             initial_states,\n",
    "                                             go_backwards=self.go_backwards,\n",
    "                                             constants=constants,\n",
    "                                             input_length=input_shape[1])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def step(self, x_input, states):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        en_seq = states[-1]\n",
    "        _, [h, c] = super(PointerLSTM, self).step(x_input, states[:-1])\n",
    "        \n",
    "        # vt*tanh(W1*e+W2*d)\n",
    "        dec_seq = K.repeat(h, input_shape[1])\n",
    "        Eij = time_distributed_dense(en_seq, self.W1, output_dim=1)\n",
    "        Dij = time_distributed_dense(dec_seq, self.W2, output_dim=1)\n",
    "        U = self.vt * tanh(Eij + Dij)\n",
    "        U = K.squeeze(U, 2)\n",
    "        \n",
    "        # make probability tensor\n",
    "        pointer = softmax(U)\n",
    "        return pointer, [h, c]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        # output shape is not affected by the attention component\n",
    "        return (input_shape[0], input_shape[1], input_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prepared the data for travelling sales man problem using this algo https://gist.github.com/mlalevic/6222750.\n",
    "Generated around 10^5 examples as input dataset.\n",
    "\n",
    "The output results though exciting werent very impressive. I am running more epochs to see if that changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open(\"data.pkl\", 'rb')\n",
    "X,Y = cPickle.load(f)\n",
    "\n",
    "hidden_size = 512\n",
    "seq_len = 11\n",
    "nb_epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "main_input = Input(shape=(seq_len, 2), name='main_input')\n",
    "\n",
    "encoder = LSTM(output_dim = hidden_size, return_sequences = True, name=\"encoder\")(main_input)\n",
    "decoder = PointerLSTM(hidden_size, output_dim = hidden_size, name=\"decoder\")(encoder)\n",
    "\n",
    "model = Model(input=main_input, output=decoder)\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X,Y, nb_epoch=nb_epochs, batch_size=8000,callbacks=[LearningRateScheduler(scheduler),])\n",
    "model.save_weights('model_weight_100.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
